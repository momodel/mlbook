---
interact_link: content/C:\Users\lbj\Desktop\book\content\11/11-6.ipynb
kernel_name: python3
has_widgets: false
title: '11-6 使用 SVM'
prev_page:
  url: /11/11-5
  title: '11-5 核技巧 II'
next_page:
  url: /12/12-1
  title: '12-1 非监督学习介绍'
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

### 使用 SVM

+ 使用 SVM的一些注意事项
+ 核函数的选择

**使用 SVM的一些注意事项** 

强烈建议使用高优化软件库，比如liblinear 和 libsvm等，来实现SVM算法。

**核函数的选择**  

在高斯核函数之外我们还有其他一些选择，如： 
+ 多项式核函数（Polynomial Kernel） 
+ 字符串核函数（String kernel） 
+ 卡方核函数（ chi-square kernel） 
+ 直方图交集核函数（histogram intersection kernel） 
这些核函数的目标也都是根据训练集和地标之间的距离来构建新特征，这些核函数需要
满足 Mercer's 定理，才能被支持向量机的优化软件正确处理。 


**多分类问题**   

假设我们利用之前介绍的一对多方法来解决一个多类分类问题。如果一共有 k 个类，则
我们需要 k 个模型，以及 k 个参数向量 θ。我们同样也可以训练 k 个支持向量机来解决多类分类问题。但是大多数支持向量机软件包都有内置的多类分类功能，我们只要直接使用即可。
 
 尽管你不去写你自己的 SVM（支持向量机）的优化软件，但是你也需要做几件事： 
1. 是提出参数 C 的选择。我们在之前的视频中讨论过误差/方差在这方面的性质。 
2. 你也需要选择内核参数或你想要使用的相似函数，其中一个选择是：我们选择不需要任何内核参数，没有内核参数的理念，也叫线性核函数。


**选择机器学习模型的一些准则** 

n 为特征数，m 为训练样本数。 
1. 如果相较于 m 而言，n 要大许多，即训练集数据量不够支持我们训练一个复杂的非
线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。 

2. 如果 n 较小，而且 m 大小中等，例如 n 在 1-1000 之间，而 m 在 10-10000 之间，使用高斯核函数的支持向量机。 

3. 如果 n 较小，而 m 较大，例如 n 在 1-1000 之间，而 m 大于 50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。 

值得一提的是，神经网络在以上三种情况下都可能会有较好的表现，但是训练神经网络
可能非常慢，选择支持向量机的原因主要在于它的代价函数是凸函数，不存在局部最小值。
