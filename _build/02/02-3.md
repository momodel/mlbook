---
interact_link: content/C:\Users\lbj\Desktop\book\content\02/02-3.ipynb
kernel_name: python3
has_widgets: false
title: '02-3 理解代价函数 - I'
prev_page:
  url: /02/02-2
  title: '02-2 代价函数'
next_page:
  url: /02/02-4
  title: '02-4 理解代价函数 - II'
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

### 理解代价函数

**代价函数的直观理解 I**

**本节强烈建议观看视频理解，以下仅列出关键步骤。**

回顾上文，我们已经得到了单线性回归的模型：

hypothesis(假设):    
         $h_{0}(x) = \theta_{0} + \theta_{1}x$     

parameters（参数）:      
$\theta_{0}$ , $\theta_{1}$   
通过选择不同的参数,会得到不同的直线拟合.

Cost Function(代价函数）:     
$J(\theta_{0}, \theta_{1}) = \frac{1}{2m}\sum_{i=1}^m(h_{0}(x^{(i)})-y^{(i)})^{2}$

Goal:     
$minimize_{\theta_{0},\theta_{1}}J(\theta_{0}, \theta_{1})$   
我们的优化目标是,最小化代价函数

为理解方便，本节讲解中将h函数简化为$h_{0}(x) =\theta_{1}x$，即设$\theta_{0}=0$

首先明确一个概念：
+ 在假设h中，$\theta_{1}$是一个固定的参数，只有$x$才是自变量。因变量为预测值 $h_{0}(x)$
+ 在优化函数$J(\theta)$中，$\theta_{1}$是自变量，因变量为预测值$h_{0}(x)$与真实值$y$的误差$J(\theta)$


从这里开始,用实例来说明,不同的 $\theta_{1}$ , 代价函数的值.建议查看视频**理解代价函数**

由此我们可以画出假设h和优化函数$J(\theta)$对应的函数图像。

![](https://i.loli.net/2018/12/04/5c0652fe1637f.png)


**牛刀小试**

Todo: 当 $\theta_{1}$ 是 0 的时候, $J(\theta)$ 的值是多少?


<span class='md-hint-alone-link pop 0'>查看答案</span>
