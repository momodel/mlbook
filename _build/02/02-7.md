---
interact_link: content/C:\Users\lbj\Desktop\book\content\02/02-7.ipynb
kernel_name: python3
has_widgets: false
title: '02-7 线性回归中的梯度下降'
prev_page:
  url: /02/02-6
  title: '02-6 理解梯度下降'
next_page:
  url: /03/features
  title: '第三章 线性代数回顾(Linear Algebra Review)'
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

### 线性回归中的梯度下降

**这一节我们主要学习以下内容**

+ 结合梯度下降和代价函数，完成线性回归算法建模

**线性回归中的梯度下降**

梯度下降是很常用的算法，它被用在很多学习模型上，包括在不限于线性回归和逻辑回归等等。

在这段视频中，我们要将梯度下降应用于具体的拟合直线的线性回归算法里。 

梯度下降算法和线性回归算法，如图：

![](https://i.loli.net/2018/11/30/5c00cf2af35aa.png)

 
对我们之前的线性回归问题运用梯度下降法，关键在于求出代价函数的导数，即： 
 $$\frac{\partial}{\partial{\theta_{j}}}J(\theta_{0}, \theta_{1}) = \frac{\partial}{\partial{\theta_{j}}}\frac{1}{2m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^{2}$$
  
 $$j=0 时：   \frac{\partial}{\partial{\theta_{0}}}J(\theta_{0}, \theta_{1}) = \frac{1}{m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})$$
 
  
$$j=1 时：\frac{\partial}{\partial{\theta_{1}}}J(\theta_{0}, \theta_{1}) = \frac{1}{m}\sum_{i=1}^m((h_{\theta}(x^{(i)})-y^{(i)})*x^{(i)})$$
 
则算法改写成： 

![](https://i.loli.net/2018/11/30/5c00d3aacf68e.png)
 

**线性回归中的梯度下降细节**

我们刚刚使用的算法，被称为**批量梯度下降**。它指的是在梯度下降的每一步中，都用到了所有的训练样本，在计算微分求导项时，需要进行求和运算。所以，在每一个单独的梯度下降中，都需要对所有m个训练样本求和。而事实上，也有其他类型的梯度下降法每次只关注训练集中的一些小的子集，后面会介绍。 

但就目前而言，应用刚刚学到的算法，你应该已经掌握了批量梯度算法，并且能把它应用到线性回归中了，这就是用于线性回归的梯度下降法。 

线性代数中，有一种计算代价函数J最小值的数值解法，**正规方程(normal equations)**，不需要梯度下降这种迭代算法（后续会讲到）。但是处理大型数据集时，梯度下降是更好的方法。

**牛刀小试**

Todo: 批量梯度下降,在实际使用中会不会遇到什么问题呢?写下你的思考



<span class='md-hint-alone-link pop 0'>查看答案</span>

