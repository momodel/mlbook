---
interact_link: content/C:\Users\lbj\Desktop\book\content\06/06-4.ipynb
kernel_name: python3
has_widgets: false
title: '06-4 正则化与逻辑回归'
prev_page:
  url: /06/06-3
  title: '06-3 正则化与线性回归'
next_page:
  url: /07/features
  title: '第七章 神经网络-表述(Neural Networks-Representation)'
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

### 正则化与逻辑回归

针对逻辑回归问题，我们在之前的课程已经学习过两种优化算法：梯度下降法，更高级的优化算法需要你自己设计代价函数$J(\theta)$。 
 
给代价函数增加一个正则化的表达式，得到代价函数:
$$
    J(\theta) = \frac{1}{m}\sum^m_{i=1}[-y^{(i)}log(h_\theta(x^{(i)}))-(1-y^{(i)}log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum_{j=1}^n\theta^2_j
$$

**牛刀小试**

<div markdown="1" class="cell code_cell">
<div class="input_area" markdown="1">
```python
def sigmoid(x, derivative=False):
    sigm = 1. / (1. + np.exp(-x))
    if derivative:
        return sigm * (1. - sigm)
    return sigm

# 代码实现
import numpy as np
def costReg(theta, X, y, learningRate):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y)
    # todo 实现代价函数第一部分的计算
    first = np.multiply(-y, np.log(sigmoid(X * theta.T))) 
    # todo 实现代价函数第二部分的计算
    second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T))) 
    # todo 实现代价函数正则化的计算
    reg = (learningRate / 2 * len(X)) * np.sum(np.power(theta[:,1:theta.shape[1]], 2)) 
    return np.sum(first - second) / (len(X)) + reg

```
</div>

</div>

![](http://imgbed.momodel.cn//20190427094457.png)


注：看上去同线性回归一样，但是由于假设$h_\theta(x)=g(\theta^TX)$，所以与线性回归不同。

注意： 
1. 虽然正则化的逻辑回归中的梯度下降和正则化的线性回归中的表达式看起来一样，但 由于两者的$h_\theta(x)$不同所以还是有很大差别。 
2. $\theta_0$不参与其中的任何一个正则化。 
 
