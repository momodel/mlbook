---
interact_link: content/C:\Users\lbj\Desktop\book\content\04/04-2.ipynb
kernel_name: python3
has_widgets: false
title: '04-2 多变量梯度下降'
prev_page:
  url: /04/04-1
  title: '04-1 多变量线性回归'
next_page:
  url: /04/04-3
  title: '04-3 梯度下降 - 特征缩放'
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

### 多变量梯度下降


与单变量线性回归类似，在多变量线性回归中，我们也构建一个代价函数，则这个代价
函数是所有建模误差的平方和，即： 
$$J(\theta_{0}, \theta_{1}...\theta_{n}) = \frac{1}{2m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^{2}$$
其中： $$h_{\theta}(x) = \theta^TX = \theta_{0}x_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + ...  + \theta_{n}x_{n}$$ 
我们的目标和单变量线性回归问题中一样，是要找出使得代价函数最小的一系列参数。
 
相应地，多变量线性回归的模型如下图：

![](https://i.loli.net/2018/11/30/5c00f9e8220d8.png)


对梯度下降求导数后得到： 

![](https://i.loli.net/2018/11/30/5c00fb2817d22.png)


我们开始随机选择一系列的参数值，计算所有的预测结果后，再给所有的参数一个新的值，如此循环直到收敛。 

**实现多变量的 cost function**

<div markdown="1" class="cell code_cell">
<div class="input_area" markdown="1">
```python
# 代价函数的python代码实现
def Cost(X, y, theta):
    inner = np.power(((X * theta.T) - y), 2)
    return np.sum(inner) / (2 * len(X))
```
</div>

</div>
